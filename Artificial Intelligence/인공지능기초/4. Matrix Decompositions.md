# Matrix Decompositions

## 4.0 Introduction  

### 4.0.1 Matrix Decomposition (matrix factorization)?  
- 두 개의 선형 방정식이 있다.

![캡처](https://user-images.githubusercontent.com/32921115/99868034-32df8c00-2c02-11eb-8301-5471ab8a71bd.PNG)

- Matrix A가 있을 때 2 or 더 많은 factor matrices로 쪼갬!

![캡처](https://user-images.githubusercontent.com/32921115/99868044-573b6880-2c02-11eb-8e41-e65239670f12.PNG)


### 4.0.2 Matrix decomposition in Machine Learning  

#### Principal Component Analysis (PCA)  
- Data를 몇 개의 축으로 표현 하는 것  
- X는 Covariance Matrix (분산 매트릭스)  
- 2x2를 Matrix decomposition을 하면 eigen-value, eigen-vector 2개가 나옴.  
- 그 중 vector, value가 큰 녀석의 쌍이 데이터를 잘 나타내는 축이 된다.  
![캡처](https://user-images.githubusercontent.com/32921115/99868289-936fc880-2c04-11eb-977e-4031cf6d4803.PNG)

#### Term-document Matrix  
- 문서를 Vector로 표현  
- 각 단어의 빈도수 (Term Frequency)  
- row의 개수는 단어의 개수  
- column의 개수는 document의 개수  
- D = 단어, N = docu, i = 단어의 idx, j = 문서의 idx, idf(i) = i번째 word를 포함하는 docu의 수  
- idf가 커질 수록, 문서의 중요도는 떨어짐 (모든 문서에 word가 등장한다는 소리)  
- 문서를 단어들의 frequecy로 표현하는데, 조금 더 의미있는 값을 부각시키는 것을 **tf idf represent** 라고 함.  
- 고차원의 Dimension vector는 다루기 어렵다. **Matrix Decomposition**으로 low dimension으로 Projection 해야 한다. (PCA)  

![캡처](https://user-images.githubusercontent.com/32921115/99868472-20675180-2c06-11eb-8894-e87817aab658.PNG)

**※ Decomposition을 통해 문서나 용어의 비교를 쉽게 할수 있다!** 
![캡처](https://user-images.githubusercontent.com/32921115/99868528-a388a780-2c06-11eb-999c-f2327fdc4c3e.PNG)

#### Collaborative prediction for recommendation  
- 고객 - 영화간 신뢰도를 나타낸 Matrix를 Decompositon해 각각 고객, 영화 Matrix로 쪼갠다.  
- 각각 vector의 내적의 값이 크면 점수가 높다는 소리!  
- 값이 높은 순으로 추천하면 됨
![캡처](https://user-images.githubusercontent.com/32921115/99868660-b8196f80-2c07-11eb-993d-612cc6c54542.PNG)

### 4.0.3 Data covariance matrix
- Data Covariance Matrix : Data 분포 (data 변수(feature) 간의 분산)를 N x N Matrix로 표현한 것  
ex) 2개의 변수를 갖는 데이터가 있다. 이 data 분포 Matrix들을 2 x 2 Matrix로 만들고 싶다!  
![캡처](https://user-images.githubusercontent.com/32921115/99868758-c1570c00-2c08-11eb-9d86-c36392e880e1.PNG)
- Cov(A,B)가 양수면 A를 좋아하면, B도 좋아한다는 것을 의미함. 
- Cov(A,B)가 음수면 A를 좋아하면, B를 싫어한다는 것을 의미함. 
![캡처](https://user-images.githubusercontent.com/32921115/99868821-55c16e80-2c09-11eb-8a3f-f6302e779036.PNG)

## 4.1 Determinant & Trace  

### 4.1.1 Determinant  
- N x N인 A의 **determinant**는 A의 column vector로 span되는 평행사변형 공간의 Volume (실수).  
- 2 x 2에서는 넓이가 되겠고, 3 x 3에서는 부피가 된다.  
- 4차원 이상에서는 상상하기 힘들다.  
![캡처](https://user-images.githubusercontent.com/32921115/99868891-ffa0fb00-2c09-11eb-9193-215c66b3111f.PNG)

**determinant example**  
![캡처](https://user-images.githubusercontent.com/32921115/99868956-9077d680-2c0a-11eb-8a62-565330c1c30b.PNG)

### 4.1.2 Properties of determinant  
- N x N Matrix A의 Det가 0이 아니면 역행렬을 구할 수 있다! -> Det(A) != 0  
- det(A) != 0 이면 A는 **full rank**고 rank(A) = n이다.  

### 4.1.3 Trace  
- Diagonal Entries of A (대각성분을 다 더한 것)
![캡처](https://user-images.githubusercontent.com/32921115/99869013-11cf6900-2c0b-11eb-86d0-94b4f2060f2c.PNG)

## 4.3 Eigendecomposition : Eigenvalues & Eigenvectors
### 4.3.2 Eigendecomposition for symmetric matrices  
- A = USU^-1 (**단 A는 Square Matrix**)  
- 여기서 U는 eigen vector들이 column vectors로 이루어져있음.  
- S는 Eigenvalue Matrix (대각성분이 eigen value 나머지 대각성분은 0)  

ex) A = [2 1  
         1 2]
         
1. eigen value와 eigen vector를 구한다. (각각 2개가 나옴)    
2. A = USU^-1을 구한다.  

### 머신 러닝에서 어떻게 사용이 될까?  
- Matrix X에서 하나의 column vector가 data를 나타낸다고 가정하자.  
- N = number of data (가로)  
- D = number of feature (세로)  
- X = UV^T로 표현 ->**Representation Learning**에서 사용. 
- 새로운 Feature를 배우는 것을 Representation Learning이라 한다.  
![캡처](https://user-images.githubusercontent.com/32921115/100427944-940dd080-30d6-11eb-85d1-e5bcdc7fd5b6.PNG)
- 사진에서 기존의 feature인 x1,x2에서는 **decision boundary**를 구하기 어렵지만, r과 theta (latent Feature)로 이루어진 feature에서는 **decision boundary**를 구하기 쉬워짐!  
- Data가 적절하게 바뀐 (coordinated space)에서 표현하면 Data 분석이 쉬워진다!  
- **Matrix Decomposition**은 **Representation Learning**의 Solution 중 하나다.  
- **Matrix Decomposition**로 **Latent Feature**를 찾아 좋은 Representation으로 Data 분석을 쉽게 할 수 있다. ML의 중요 분야중 하나.  

## 4.4 Singular Value Decomposition (SVD)  
- 모든 Matrix에 대해 Decomposition 가능  
- XX^T를 USV^T로 쪼갠다.  
- U는 m x m, S는 m x n, V^T는 n x n  
- XX^T와 X^TX의 eigen value는 같다.  
- S에는 eigen value에 루트를 씌운 값 (Singular Value)이 대각성분에 들어감. 이 대각 성분들은 큰 수로 Sorting.  
- S의 **Rank는 min (D,N)**  

### 4.4.2 Singular Value Decomposition (SVD)  
![image](https://user-images.githubusercontent.com/32921115/100431638-cc63dd80-30db-11eb-8202-d9c89fc3ce1a.png)

