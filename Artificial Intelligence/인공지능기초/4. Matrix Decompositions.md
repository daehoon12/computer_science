# Matrix Decompositions

## 4.0 Introduction  

### 4.0.1 Matrix Decomposition (matrix factorization)?  
- 두 개의 선형 방정식이 있다.

![캡처](https://user-images.githubusercontent.com/32921115/99868034-32df8c00-2c02-11eb-8301-5471ab8a71bd.PNG)

- Matrix A가 있을 때 2 or 더 많은 factor matrices로 쪼갬!

![캡처](https://user-images.githubusercontent.com/32921115/99868044-573b6880-2c02-11eb-8e41-e65239670f12.PNG)


### 4.0.2 Matrix decomposition in Machine Learning  

#### Principal Component Analysis (PCA)  
- Data를 몇 개의 축으로 표현 하는 것  
- X는 Covariance Matrix (분산 매트릭스)  
- 2x2를 Matrix decomposition을 하면 eigen-value, eigen-vector 2개가 나옴.  
- 그 중 vector, value가 큰 녀석의 쌍이 데이터를 잘 나타내는 축이 된다.  
![캡처](https://user-images.githubusercontent.com/32921115/99868289-936fc880-2c04-11eb-977e-4031cf6d4803.PNG)

#### Term-document Matrix  
- 문서를 Vector로 표현  
- 각 단어의 빈도수 (Term Frequency)  
- row의 개수는 단어의 개수  
- column의 개수는 document의 개수  
- D = 단어, N = docu, i = 단어의 idx, j = 문서의 idx, idf(i) = i번째 word를 포함하는 docu의 수  
- idf가 커질 수록, 문서의 중요도는 떨어짐 (모든 문서에 word가 등장한다는 소리)  
- 문서를 단어들의 frequecy로 표현하는데, 조금 더 의미있는 값을 부각시키는 것을 **tf idf represent** 라고 함.  
- 고차원의 Dimension vector는 다루기 어렵다. **Matrix Decomposition**으로 low dimension으로 Projection 해야 한다. (PCA)  

![캡처](https://user-images.githubusercontent.com/32921115/99868472-20675180-2c06-11eb-8894-e87817aab658.PNG)
