# Matrix Decompositions

## 4.0 Introduction  

### 4.0.1 Matrix Decomposition (matrix factorization)?  
- 두 개의 선형 방정식이 있다.

![캡처](https://user-images.githubusercontent.com/32921115/99868034-32df8c00-2c02-11eb-8301-5471ab8a71bd.PNG)

- Matrix A가 있을 때 2 or 더 많은 factor matrices로 쪼갬!

![캡처](https://user-images.githubusercontent.com/32921115/99868044-573b6880-2c02-11eb-8e41-e65239670f12.PNG)


### 4.0.2 Matrix decomposition in Machine Learning  

#### Principal Component Analysis (PCA)  
- Data를 몇 개의 축으로 표현 하는 것  
- X는 Covariance Matrix (분산 매트릭스)  
- 2x2를 Matrix decomposition을 하면 eigen-value, eigen-vector 2개가 나옴.  
- 그 중 vector, value가 큰 녀석의 쌍이 데이터를 잘 나타내는 축이 된다.  
![캡처](https://user-images.githubusercontent.com/32921115/99868289-936fc880-2c04-11eb-977e-4031cf6d4803.PNG)

#### Term-document Matrix  
- 문서를 Vector로 표현  
- 각 단어의 빈도수 (Term Frequency)  
- row의 개수는 단어의 개수  
- column의 개수는 document의 개수  
- D = 단어, N = docu, i = 단어의 idx, j = 문서의 idx, idf(i) = i번째 word를 포함하는 docu의 수  
- idf가 커질 수록, 문서의 중요도는 떨어짐 (모든 문서에 word가 등장한다는 소리)  
- 문서를 단어들의 frequecy로 표현하는데, 조금 더 의미있는 값을 부각시키는 것을 **tf idf represent** 라고 함.  
- 고차원의 Dimension vector는 다루기 어렵다. **Matrix Decomposition**으로 low dimension으로 Projection 해야 한다. (PCA)  

![캡처](https://user-images.githubusercontent.com/32921115/99868472-20675180-2c06-11eb-8894-e87817aab658.PNG)

**※ Decomposition을 통해 문서나 용어의 비교를 쉽게 할수 있다!** 
![캡처](https://user-images.githubusercontent.com/32921115/99868528-a388a780-2c06-11eb-999c-f2327fdc4c3e.PNG)

#### Collaborative prediction for recommendation  
- 고객 - 영화간 신뢰도를 나타낸 Matrix를 Decompositon해 각각 고객, 영화 Matrix로 쪼갠다.  
- 각각 vector의 내적의 값이 크면 점수가 높다는 소리!  
- 값이 높은 순으로 추천하면 됨
![캡처](https://user-images.githubusercontent.com/32921115/99868660-b8196f80-2c07-11eb-993d-612cc6c54542.PNG)

### 4.0.3 Data covariance matrix
- Data Covariance Matrix : Data 분포 (data 변수(feature) 간의 분산)를 N x N Matrix로 표현한 것  
ex) 2개의 변수를 갖는 데이터가 있다. 이 data 분포 Matrix들을 2 x 2 Matrix로 만들고 싶다!  
![캡처](https://user-images.githubusercontent.com/32921115/99868758-c1570c00-2c08-11eb-9d86-c36392e880e1.PNG)
- Cov(A,B)가 양수면 A를 좋아하면, B도 좋아한다는 것을 의미함. 
- Cov(A,B)가 음수면 A를 좋아하면, B를 싫어한다는 것을 의미함. 
![캡처](https://user-images.githubusercontent.com/32921115/99868821-55c16e80-2c09-11eb-8a3f-f6302e779036.PNG)

## 4.1 Determinant & Trace  

### 4.1.1 Determinant  
- N x N인 A의 **determinant**는 A의 column vector로 span되는 평행사변형 공간의 Volume (실수).  
- 2 x 2에서는 넓이가 되겠고, 3 x 3에서는 부피가 된다.  
- 4차원 이상에서는 상상하기 힘들다.  
![캡처](https://user-images.githubusercontent.com/32921115/99868891-ffa0fb00-2c09-11eb-9193-215c66b3111f.PNG)

