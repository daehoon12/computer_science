# Matrix Decompositions

## 4.0 Introduction  

### 4.0.1 Matrix Decomposition (matrix factorization)?  
- 두 개의 선형 방정식이 있다.

![캡처](https://user-images.githubusercontent.com/32921115/99868034-32df8c00-2c02-11eb-8301-5471ab8a71bd.PNG)

- Matrix A가 있을 때 2 or 더 많은 factor matrices로 쪼갬!

![캡처](https://user-images.githubusercontent.com/32921115/99868044-573b6880-2c02-11eb-8e41-e65239670f12.PNG)


### 4.0.2 Matrix decomposition in Machine Learning  

#### Principal Component Analysis (PCA)  
- Data를 몇 개의 축으로 표현 하는 것  
- X는 Covariance Matrix (분산 매트릭스)  
- 2x2를 Matrix decomposition을 하면 eigen-value, eigen-vector 2개가 나옴.  
- 그 중 vector, value가 큰 녀석의 쌍이 데이터를 잘 나타내는 축이 된다.  
![캡처](https://user-images.githubusercontent.com/32921115/99868289-936fc880-2c04-11eb-977e-4031cf6d4803.PNG)

#### Term-document Matrix  
- 문서를 Vector로 표현  
- 각 단어의 빈도수 (Term Frequency)  
- row의 개수는 단어의 개수  
- column의 개수는 document의 개수  
- D = 단어, N = docu, i = 단어의 idx, j = 문서의 idx, idf(i) = i번째 word를 포함하는 docu의 수  
- idf가 커질 수록, 문서의 중요도는 떨어짐 (모든 문서에 word가 등장한다는 소리)  
- 문서를 단어들의 frequecy로 표현하는데, 조금 더 의미있는 값을 부각시키는 것을 **tf idf represent** 라고 함.  
- 고차원의 Dimension vector는 다루기 어렵다. **Matrix Decomposition**으로 low dimension으로 Projection 해야 한다. (PCA)  

![캡처](https://user-images.githubusercontent.com/32921115/99868472-20675180-2c06-11eb-8894-e87817aab658.PNG)

**※ Decomposition을 통해 문서나 용어의 비교를 쉽게 할수 있다!** 
![캡처](https://user-images.githubusercontent.com/32921115/99868528-a388a780-2c06-11eb-999c-f2327fdc4c3e.PNG)

#### Collaborative prediction for recommendation  
- 고객 - 영화간 신뢰도를 나타낸 Matrix를 Decompositon해 각각 고객, 영화 Matrix로 쪼갠다.  
- 각각 vector의 내적의 값이 크면 점수가 높다는 소리!  
- 값이 높은 순으로 추천하면 됨
![캡처](https://user-images.githubusercontent.com/32921115/99868660-b8196f80-2c07-11eb-993d-612cc6c54542.PNG)

### 4.0.3 Data covariance matrix
- Data Covariance Matrix : Data 분포 (data 변수(feature) 간의 분산)를 N x N Matrix로 표현한 것  
ex) 2개의 변수를 갖는 데이터가 있다. 이 data 분포 Matrix들을 2 x 2 Matrix로 만들고 싶다!  
![캡처](https://user-images.githubusercontent.com/32921115/99868758-c1570c00-2c08-11eb-9d86-c36392e880e1.PNG)
- Cov(A,B)가 양수면 A를 좋아하면, B도 좋아한다는 것을 의미함. 
- Cov(A,B)가 음수면 A를 좋아하면, B를 싫어한다는 것을 의미함. 
![캡처](https://user-images.githubusercontent.com/32921115/99868821-55c16e80-2c09-11eb-8a3f-f6302e779036.PNG)

## 4.1 Determinant & Trace  

### 4.1.1 Determinant  
- N x N인 A의 **determinant**는 A의 column vector로 span되는 평행사변형 공간의 Volume (실수).  
- 2 x 2에서는 넓이가 되겠고, 3 x 3에서는 부피가 된다.  
- 4차원 이상에서는 상상하기 힘들다.  
![캡처](https://user-images.githubusercontent.com/32921115/99868891-ffa0fb00-2c09-11eb-9193-215c66b3111f.PNG)

**determinant example**  
![캡처](https://user-images.githubusercontent.com/32921115/99868956-9077d680-2c0a-11eb-8a62-565330c1c30b.PNG)

### 4.1.2 Properties of determinant  
- N x N Matrix A의 Det가 0이 아니면 역행렬을 구할 수 있다! -> Det(A) != 0  
- det(A) != 0 이면 A는 **full rank**고 rank(A) = n이다.  

### 4.1.3 Trace  
- Diagonal Entries of A (대각성분을 다 더한 것)
![캡처](https://user-images.githubusercontent.com/32921115/99869013-11cf6900-2c0b-11eb-86d0-94b4f2060f2c.PNG)

## 4.3 Eigendecomposition : Eigenvalues & Eigenvectors

### 4.3.1 Eigenvalues & Eigenvectors  
- A라는 Square Matrix가 있을 때, **람다를 A의 eigenvalue, u를 A의 eigenvector라 한다.  
- 기하학적 의미로 A는 방향만 같고 길이만 변하는 (Scale Matrix)  
![image](https://user-images.githubusercontent.com/32921115/101866752-fb3e8100-3bbc-11eb-8967-9f3b68a3eb6b.png)

### - Example -
![image](https://user-images.githubusercontent.com/32921115/101868870-92a5d300-3bc1-11eb-8166-a8d06a5e7da1.png)

1. A가 square matrix인지 확인  
2. Ax = Lamdax 꼴의 방정식을 만들고 좌변으로 옮긴다.  
3. det[A-람다I] = 0이여야 함 (벡터 x != 0)  
4. 2개의 람다 값이 나오는데 이 녀석이 eigenvalues가 된다.  
5. eigenvalues를 가지고 eigenvector를 구한다. (2번의 식에 있는 람다에 각각 대입해봄)  
6. 비례식 기반으로 eigenvector를 구한다.  
7. 검산  

※ eigenvalues는 유일한 값, but eigenvectors는 유일한 값이 나오지 않는다.

#### Geometric한 공간에서의 Eigenvalue & Eigenvectors  
- 벡터 x에 대해서 transformation 한 결과는 Ax  
- x에 eigenvector를 곱한 것과 같음  
- A는 n x n해서 n제곱의 수를 봐야하는데, 람다는 n개만 보면 됨. -> **A에 의한 transformation을 압축적으로 표현/ 설명  
- 람다를 통해 벡터를 그리면 A의 역할을 **더 직관적으로** 볼 수 있다.
- A의 차원이 더 커지면, eigenvalue, eigenvector로의 요약 필요성이 높아짐  
![image](https://user-images.githubusercontent.com/32921115/101869948-faf5b400-3bc3-11eb-9143-c5bbb4ed3adb.png)

### 4.3.2 Eigendecomposition (Spectral decomposition)  
- 아래 식을 만족하면 Matrix A는 diagonalizable 하다.  
![image](https://user-images.githubusercontent.com/32921115/101871251-9851e780-3bc6-11eb-96bd-122010546c6c.png)
- diagonal matrix : determinants, powers, inverses 연산이 빠르다!  
![image](https://user-images.githubusercontent.com/32921115/101871702-5d03e880-3bc7-11eb-89fd-80953080dc24.png)
- 이 때 U의 요소는 eigenvector로 이루어져 있다. (eigenvector matrix)  
- 마찬가지로 시그마의 대각 성분도 eigenvalue로 이루어져 있다. (eigenvalue matrix)  

### 4.3.2 Eigendecomposition for symmetric matrices  
- A가 **Symmetric**하면 eigenvectors {u1,.....un}은 **orthonormal** 하다.  
(Orthonormal : Norm의 값이 1이고, orthogonal한 벡터)   
- A = USU^-1 (**단 A는 Square Matrix**) (A가 Symmetric 할 시 U^-1 -> U^T로 바뀜)  
- 여기서 U는 eigen vector들이 column vectors로 이루어져있음.  
- S는 Eigenvalue Matrix (대각성분이 eigen value 나머지 대각성분은 0)  
#### Geometric한 공간에서의 Eigendecomposition  
![image](https://user-images.githubusercontent.com/32921115/101874052-7870f280-3bcb-11eb-9bec-f71f5d1723a7.png)

1. eigenvector를 standard basis로 mapping  
2. remap된 eigenvector를 scaling 한다. (방향은 동일하고 크기는 바뀜)  
3. basis change를 취소 (원래 basis로 돌려 놓음), original coorinate로 가져옴  

**Matrix A를 분석할 수 있음.**  


#### PCA  
![image](https://user-images.githubusercontent.com/32921115/101874605-870bd980-3bcc-11eb-87f8-ae23f071ef5b.png)

- Covariance Matrix에서 eigenvalue와 eigenvector를 구함.  
- eigenvalue의 값이 낮은 녀석을 없앰으로써 PCA를 한다.


ex) A = [2 1  
         1 2]
         
1. eigen value와 eigen vector를 구한다. (각각 2개가 나옴)    
2. A = USU^-1을 구한다.  


### 머신 러닝에서 어떻게 사용이 될까?  
- Matrix X에서 하나의 column vector가 data를 나타낸다고 가정하자.  
- N = number of data (가로)  
- D = number of feature (세로)  
- X = UV^T로 표현 ->**Representation Learning**에서 사용. 
- 새로운 Feature를 배우는 것을 Representation Learning이라 한다.  
![캡처](https://user-images.githubusercontent.com/32921115/100427944-940dd080-30d6-11eb-85d1-e5bcdc7fd5b6.PNG)
- 사진에서 기존의 feature인 x1,x2에서는 **decision boundary**를 구하기 어렵지만, r과 theta (latent Feature)로 이루어진 feature에서는 **decision boundary**를 구하기 쉬워짐!  
- Data가 적절하게 바뀐 (coordinated space)에서 표현하면 Data 분석이 쉬워진다!  
- **Matrix Decomposition**은 **Representation Learning**의 Solution 중 하나다.  
- **Matrix Decomposition**로 **Latent Feature**를 찾아 좋은 Representation으로 Data 분석을 쉽게 할 수 있다. ML의 중요 분야중 하나.  

## 4.4 Singular Value Decomposition (SVD)  
- 모든 Matrix에 대해 Decomposition 가능  
- XX^T를 USV^T로 쪼갠다.  
- U는 m x m, S는 m x n, V^T는 n x n  
- XX^T와 X^TX의 eigen value는 같다.  
- S에는 eigen value에 루트를 씌운 값 (Singular Value)이 대각성분에 들어감. 이 대각 성분들은 큰 수로 Sorting.  
- S의 **Rank는 min (D,N)**  

### 4.4.2 Singular Value Decomposition (SVD)  
![image](https://user-images.githubusercontent.com/32921115/100431638-cc63dd80-30db-11eb-8202-d9c89fc3ce1a.png)

- 시그마 : Singular value Matrix
- u1,... ud 벡터 : left-singular vector   
- v1,....vd 벡터 : right-singular vector  
- U : left-singular matrix  
- V : right-singular matrix  

#### ex) Movie ratings & its SVD (3 people, 4 movies)  
![image](https://user-images.githubusercontent.com/32921115/100433392-5b71f500-30de-11eb-8e94-462462cc13d5.png)

- SVD 도메인 벡터(영화에 따른 관객 선호도)를 영화 space vector와 viewer space vector로 해석.  
- u 벡터는 영화의 선호도  
- v 벡터는 어떤 영화의 장르를 선호하는지 판별해주는 viewer, 위 그림에서 v1는 u1과 같이 보면 **SF를 좋아하는 사람**를 반영하고 **2명의 관객이 SF를 좋아한다는 뜻**이 된다.  
- 시그마는 **순수성**을 의미한다. 예를 들면 **SF를 좋아하는 사람이 SF만 좋아하는 정도**

### 4.4.3 Low-rank Approximation  
- SVD의 Low-rank Approximation  
- Xd = Rank를 d로 제한둬서 X를 잘 표현 (X의 Approximation)  
- 큰 Singular value(시그마 1) 부터 d개까지 선택. d < **number of Singular value**  
- d개의 leading singular value, vector, 그리고 새로운 USV를 구성  
- 즉 X(d) = U(d)S(d)V(d)  
- 큰 Singular value부터 선택하는 것을 **Best rank-D approximation**이라 한다.  
- 차원이 줄어들어 **Dimensional Reduction**에 사용된다.
