# Analytic Geometry  

## 3.1 Norm & inner product  

### 3.1.1 Norm  
- Norm에 Vector Space V를 넣으면 실수가 나옴. (일종의 함수)  
- Vector의 크기를 재는 방법들.  
ex) 벡터 v [x,y,z]의 크기를 구하는 방법은, 성분을 제곱해서 루트를 씌우거나, 절댓값을 취한 뒤 더해서 구할 수 있다.  

#### 조건  

 <img src="/Artificial Intelligence/Capture/16.png" width="50%" height="50%">  

### 3.1.2 lp norm on R^n  
- Norm은 종류가 여러가지 (P는 실수)    
- 맨해튼 Norm (L1)은 물리적인 제약이 있을 때 사용하면 좋다.  

 <img src="/Artificial Intelligence/Capture/17.png" width="50%" height="50%">

### 3.1.3 Inner Product (+ Machine Learning)  
 <img src="/Artificial Intelligence/Capture/18.png" width="50%" height="50%">
 
<x,y> = x^T y  

#### Machine Learning에서의 Inner Product
- 2개의 벡터가 얼마나 비슷한지 측정하는 방법 중 하나가 Inner Product (내적)이다.  (
- 어떠한 문서들에 대한 벡터가 유사하다는 것은, 서로 비슷한 문서일 확률이 높음.  
- Similarity Measure

### 3.1.4 Distance  
- 벡터의 길이가 짧을수록 두 벡터는 비슷함.
- Disimilarity (벡터의 거리는 Similarity에 반비례함)  
- distance = '두벡터차'의 Norm  

 <img src="/Artificial Intelligence/Capture/19.png" width="50%" height="50%">

### 3.1.5 Angles between Two Vectors  

 <img src="/Artificial Intelligence/Capture/20.png" width="50%" height="50%">
 
## 3.2 Orthogonality  

### 3.2.1 Orthogonality  
- <x,y>의 내적이 0이면 **Orthogonality**.  
- Orthogonality하고 norm x == norm y == 1이면 **Orthonormal**.  
- Orthogonal Matrix (직교 행렬)은 AA^T=I면 된다. 즉 A^-1 = A^T  

### 3.2.2 Orthgonal Transfomation  
- A가 O.M이면 x가 rotation (길이 그대로 유지). 각도만 바뀐다.  
 <img src="/Artificial Intelligence/Capture/21.png" width="50%" height="50%">

### 3.2.3 Linear Orthogonal Transform  
- Ax = x1a1 + x2a2... xnan으로 표현 할 때 ,a의 값은 다 orthonormal basis vector다.  
- 그러면 b도 마찬가지로 x1a1 + .... xnan으로 표현이 가능하다. 
- ML에서는 b를 training으로 배운다.
(사진)

### 3.2.4 Eigenface  
- Task = 사람 얼굴을 Model에 넣어 A or B or C 인지 분류하는 일.  
- 높은 차원을 낮은 차원으로 낮춰줌 (PCA)  
- Components = basis,  
- 265 * 256 -> (256 * 256, 1)   
- basis는 고정하고 x값은 바뀜 (Coefficient).  
(사진)

### 3.2.5 Gram-Schmidt Orthogonalization    
- 어떤 vector space에 대한 basis를 구함.  
- 임의의 벡터 A를 Span하는 basis 구하기.  
(사진)
