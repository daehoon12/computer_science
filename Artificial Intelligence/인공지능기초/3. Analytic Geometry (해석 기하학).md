# Analytic Geometry  

## 3.1 Norm & inner product  

### 3.1.1 Norm  
- Norm에 Vector Space V를 넣으면 실수가 나옴. (일종의 함수)  
- Vector의 크기를 재는 방법들.  
ex) 벡터 v [x,y,z]의 크기를 구하는 방법은, 성분을 제곱해서 루트를 씌우거나, 절댓값을 취한 뒤 더해서 구할 수 있다.  

#### 조건  

 <img src="/Artificial Intelligence/Capture/17.png" width="50%" height="50%">  

### 3.1.2 lp norm on R^n  
<img src="/Artificial Intelligence/Capture/18.png" width="50%" height="50%">  

- Norm은 종류가 여러가지 (P는 실수)    
- 맨해튼 Norm (L1)은 물리적인 제약이 있을 때 사용하면 좋다.  

### 3.1.3 Inner Product (+ Machine Learning)  
 <img src="/Artificial Intelligence/Capture/19.png" width="50%" height="50%">  
![image](https://user-images.githubusercontent.com/32921115/101982451-55bc0800-3cb7-11eb-8d6f-c30ab6eb80b3.png)

<x,y> = x^T y  


#### Machine Learning에서의 Inner Product
- **2개의 벡터가 얼마나 비슷한지 측정하는 방법 중 하나**가 Inner Product (내적)이다.  
- 어떠한 문서들에 대한 벡터가 유사하다는 것은, 서로 비슷한 문서일 확률이 높음.  
- Similarity Measure  

#### Cosine Similarity  
- 두 벡터 간의 코사인 각도를 이용하여 구할 수 있는 두 벡터의 유사도  
- 두 벡터가 같으면 Cos의 값은 1, 직각일 경우 0, 아예 다를 경우 -1을 가짐  

![image](https://user-images.githubusercontent.com/32921115/101982480-8b60f100-3cb7-11eb-8fe5-3c0e74bf7608.png)

### 3.1.4 Distance  
- 벡터의 길이가 짧을수록 두 벡터는 비슷함.
- Disimilarity (벡터의 거리는 Similarity에 반비례함)  
- distance = '두벡터차'의 Norm  

 <img src="/Artificial Intelligence/Capture/20.png" width="50%" height="50%">

### 3.1.5 Angles between Two Vectors  

 <img src="/Artificial Intelligence/Capture/21.png" width="50%" height="50%">
 
## 3.2 Orthogonality  

### 3.2.1 Orthogonality  
- <x,y>의 내적이 0이면 **Orthogonality**.  
- Orthogonality하고 norm x == norm y == 1 (x와 y의 크기가 1) 이면 **Orthonormal**.  
- Orthogonal Matrix (직교 행렬)은 AA^T=I면 된다. 즉 A^-1 = A^T  
- Orthogonal Matrix (직교 행렬)은 **Column Vector** 간 **Orthonormal**한 성질을 가진다. (내적이 0, 길이가 1)  


### 3.2.2 Orthgonal Transfomation  
- A가 O.M이면 x가 rotation (길이 그대로 유지). 각도만 바뀐다.  
- L2 Norm의 형태를 보존함  
- Orthonormal

<img src="/Artificial Intelligence/Capture/26.png" width="50%" height="50%">

### 3.2.3 Linear Orthogonal Transform  
- Ax = x1a1 + x2a2 +.... +xnan 으로 표현 할때 모든 a가 orthonormal basis vector면 b = x1a1 +... +xnan으로 표현이 가능하다.  
- ML에서는 어떤 Training set에서 특정 조건을 만족하는 basis를 스스로 찾게 한다.
<img src="/Artificial Intelligence/Capture/22.png" width="50%" height="50%">

### 3.2.4 Eigenface  
- components = basis vectors (사진)    
- coefficient = x (Feature)
- (256, 256) Matrix를 -> (256 곱 256, 1)로 만든 뒤 PCA를 통해 차원을 낮춤.

<img src="/Artificial Intelligence/Capture/23.png" width="50%" height="50%">

### 3.2.5 Gram-Schmidt Orthogonalization  
- a1과 a2의 contribution한 벡터를 모으면 basis가 된다
<img src="/Artificial Intelligence/Capture/24.png" width="50%" height="50%">
<img src="/Artificial Intelligence/Capture/25.png" width="50%" height="50%">

## 3.3 Orhogonal Projections (정사영)  

### 3.3.1 Orhogonal Projections  
- Goals : 2D -> 1D, [x1,x2] -> z, High D -> low D, 원본 정보 손실을 최소화.  
- 거리가 짧을수록 데이터를 잘 설명하는 축으로 Projection 한 것이다.  
- 사진의 그래프는 unlabled data 그래프 (feature가 x축 y축에 있음.), linear regression과의 차이점.  
- PCA : **차원 축소**를 위해 **Optimal Orthogonal Transformation**을 정의함.  

### 3.3.2 Hyperplane & Curved Manifold  
- 2차원 : Plane, 다차원 : Manifold  

### 3.3.3 Projections & Marchine Learning  
(사진)

### 3.3.4 Projection onto a line  
- 벡터 x와 v의 유닛벡터를 내적함. : x를 v로 proj 했을 때 길이. (T를 넣는 이유는 column vector여서)  
- 나온 결과에 다시 v의 유닛 벡터를 곱함 (v의 방향이 필요)  
(사진)

### 3.3.5 Projection in a Higher - Dimensions  


## 3.4 Rotations  
In R^2  
- G = [cos세타, -sin세타
      sin세타 , cos세타]

- Linear transform 중에서 length가 변하지 않는 것은 **Orthogonal Transform** 뿐이다.  
- A^T = A^-1  

(사진)  
