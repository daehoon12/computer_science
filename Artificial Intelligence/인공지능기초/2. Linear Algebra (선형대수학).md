# 1. Machine Learning (기계 학습)  
- 명시적인 규칙 없이 기계가 데이터로부터 학습하도록 하는 기술  

  <img src="/Artificial Intelligence/Capture/1.PNG" width="50%" height="50%">  
  <br/>
# 2. Linear Algebra (선형대수학)  
- 우리는 집의 크기와 가격간의 관계를 찾기를 원함.  
- a : house size, b : price  (data)  
- a에 인자가 두개가 있는 이유 : 집 값에 영향을 주는 요인 (집 평수, 방의 수, 지하철과의 거리 등)
- x1 (x 절편) = slop, x2 (y 절편) = intercept  
- In 기계 학습, a,b : Date example, x : feature.  

## 2.0 Introduction  
- Linear Regression (선형 회귀)   
  - 이 예제에서는 집 값과 집의 크기의 상관 관계를 유추하는 기법  
  - 사진에서 붉은 선 : 모델, X : 데이터
  - 새로운 Data에 대해 예측 (Inference 가능)  
<img src="/Artificial Intelligence/Capture/2.PNG" width="50%" height="50%">  
<br/>

### 2.0.1 Motivating Example  
- Parameter : 모델을 결정짓는 중요한 키 값. Ax =b에서 x에 속함.  
<img src="/Artificial Intelligence/Capture/3.PNG" width="50%" height="50%">  
<br/>

### 2.0.2 Linear Algebra  
 - Algebra (대수학) : 수학적인 기호와 수학적인 기호를 제어하는 학문.  
 ex) bx +c =0
 
 - Linear Algebra (선형대수학) : 선형식(Ax = b, A는 Matrix, b는 Vector)와 관련, 벡터와 벡터들을 제어하는 규칙  

 ### 2.0.3 Two important equations in linear algebra  
 - Ax = b (linear equation)  
 - Ax = Tx (eigenvalue equation)  
 
### 2.0.4 Why Linear Algebra for ML?
 - 데이터는 벡터와 행렬로 표현된다.  
 - 모델은 linear equation (Ax = b)으로 표현이 된다. 
 
### 2.0.5 Mathematical Objects in Linear Algebra  
 - Scalar (R): x  
 - Vector (R^n): x (x i)
 - Matrix (R^(m x n)): X (X i,j)  
 - Tensor (R^(m x n x k)): X (X i,j,k) 
 
### 2.1.1 Vectors   
- In Computer Science, Vectors is list of numbers.  
- 주로 Column Vector 사용.  
<img src="/Artificial Intelligence/Capture/4.png" width="50%" height="50%">  

### 2.1.2 Vector Space  
- vector들의 Set.  
- 실수에서 덧셈과 스칼라 곱에 닫혀있다. (ex 실수 + 실수 = 실수 -> 덧셈에 대해 닫혀있다.)  
- **Vector addition과 Scalar multiplication 연산에 대해 닫혀있는 벡터들의 집합**  

#### Polynomials are vectors? (다항식도 벡터니?)  
<img src="/Artificial Intelligence/Capture/5.png" width="50%" height="50%"> 

### 2.1.3 Matrices  
<img src="/Artificial Intelligence/Capture/6.png" width="50%" height="50%"> 

#### 가우스, 조던 소거법  
- **Inverse Matrix**를 구하는 방법 중 한가지.
- 피벗 성분이 있는 열의 나머지 성분은 전부 0 이여야 한다.  
- 가우스 소거법 혹은 가우스-조던 소거법에서 두 행의 위치를 서로 바꿀 수 있다.  
- [A|I] -> [I|A(-1)]  

#### Symmetric Matrix
- A와 A의 전치행렬 A^T가 같다.  
- A는 Square Matrix  
- A = A^T  
- A = X(X^T) -> A^T= (XX^T)^T -> (X^T)X  

#### Positive Definite Matrix  
![image](https://user-images.githubusercontent.com/32921115/101979670-55654200-3ca2-11eb-9773-53a33c6a4f26.png)  
- Symmetric Matrix A는 X(t)Ax (스칼라 값)> 0을 만족하면, A는 **Positive definite**하다. (단 벡터 x는 0이 아니다.)  
-  Symmetric Matrix A는 X(t)Ax (스칼라 값)>= 0을 만족하면, A는 **Positive semidefinite**하다. (단 벡터 x는 0이 아니다.)  
- 제곱 꼴로 만들어서 무조건 양수가 나오는 식이 만들어지면 조건을 만족, 반례가 하나라도 있으면 만족하지 않는다.  

**Example**  
![image](https://user-images.githubusercontent.com/32921115/101979743-0f5cae00-3ca3-11eb-93e5-dfeac3d89414.png)

## 2.2 Linear independence & rank  
![image](https://user-images.githubusercontent.com/32921115/101979823-bd685800-3ca3-11eb-938c-8ef02d700cf9.png)
- a(1)c(1) + a(2)c(2)+ .... + a(k)c(k)= 0을 만족하는 상수 a(1),a(2)...a(k)가 모두 0이면 그 식은 linearly independent (선형 독립, 1차 독립)이다.

ex) R^2의 두 벡터(1,0), (0,1)이 있다고 가정.  
    C1(1,0) + C2(0,1) = (0,0)을 만족하는 c1과 c2는 0밖에 없으므로 R^2의 두 벡터 (1,0), (0,1)은 Linearl Independent.  
    
### 2.2.2. Linear independence & Machine Learning  
- Q. 왜 linear independence가 Machine Learning에서 중요한가?   
- A. 어떤 linear independent한 벡터가 있다면, 그 벡터들의 linear sum을 통해 그 공간에 어떤 벡터의 표현이 가능하다.  
- Q. Linear independence의 기하학적인 의미  
- A. <img src="/Artificial Intelligence/Capture/7.png" width="50%" height="50%"> 

### 2.2.4 Span & Basis  
- Span : 어느 벡터공간의 vector a1v1 + a2v2 + ... + anvn = v가 되는 스칼라 a1,a2... an이 존재할 경우.   
- V = Span(A) : V의 모든 벡터는 A 벡터들의 linear combination으로 표현이 가능하다.  
ex) A =[2,1] span of A (a1 [1,0], a2 [0,1]) 일 때, **Span(A) = 2a1 + a2**로 표현 가능  
- A가 꼭 independent([1,0], [0,1]) 일 필요 없다.  
- Basis : Linear Independent 하면서 V를 Span 할 때, V를 basis라고 한다.  

#### Example
<img src="/Artificial Intelligence/Capture/8.png" width="50%" height="50%"> 

#### Span의 기하학적인 의미
<img src="/Artificial Intelligence/Capture/9.png" width="50%" height="50%"> 

R^2 = 2개의 벡터가 span  
R^3 = 3개의 벡터가 span  
Span하면 모든 벡터를 좌표에 표현할 수 있다.  

### 2.2.5 Rank  
- Matrix의 각각의 column으로 만들수 있는 vector space의 차원(dimension).  
- Basis한 벡터의 갯수 -> rank  
- Matrix의 모든 column vector가 linear independent하면 **full rank**라고 한다.
- A가 **Full Rank** 이면 **역행렬 (Invertible)** 존재  

#### Example  
<img src="/Artificial Intelligence/Capture/10.png" width="50%" height="50%"> 

1. Linear independent를 따져봄.  
2. x1=x2=x3=0 뿐이면 linear independent하다.  
3. x3 = -x1 = -x2 이므로 liear independent하지 않다.  
4. 어떤 하나의 column vector가 다른 2개의 column vector로 표현 가능 or linear independent한 벡터들을 찾는다. (찾으면 Rank는 2)   
5. Rank(A) = 2  

<img src="/Artificial Intelligence/Capture/11.png" width="50%" height="50%"> 

linear independent  
linear dependent


## 2.3 Linear Equations  

### 2.3.1 Linear Regression Example  

※ **데이터를 잘 표현하는 x1과 x2의 근사 값을 찾는 것이 Linear Regreesion의 목적**  
a11*x1 + x2 = b1  

a21*x1 + x2 = b2  

a31*x1 + x2 = b3  
ex) ai1 = 집의 i번째 크기, bi = i번째 집의 가격  
- A,b의 값은 주어짐  
- 데이터의 개수는 최소 4개 이상이어야 예측 가능하다.  
- linear equation Ax = b을 푸는 것.  
- 또 다른 attribute(feature)을 추가할 수 있음. (지하철과의 거리)  

<img src="/Artificial Intelligence/Capture/12.png" width="50%" height="50%"> 

### 2.3.2 Linear Equations
- complete (m == n)  
- undercomplete (m > n) // Skinny  -> Ax = b
- overcomplete (m < n) // Fat  

### 2.3.3 Range Space and Null Space  
- Range Space (Column Space) Ax = b를 만족하는 x  
- Null Space = Ax = 0을 만족하는 x  
#### Example

<img src="/Artificial Intelligence/Capture/13.png" width="50%" height="50%"> 

R(A) : [0,0]  
N(A) : x1,x2는 모든 실수  

R(B) : [x1,0]  
N(B) : : x1은 0, x2는 모든 값  [0,x2]  

### 2.4.4 Linear Algebraic Equations  
- **Ax = b** exists if and only if **b ( R(A)**    
- B가 A의 Range Space 일때 Ax = b가 존재한다.  
